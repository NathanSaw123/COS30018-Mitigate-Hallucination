

  2%|███                                                                                                                                                                                      | 1/60 [00:35<34:40, 35.25s/it]
{'loss': 2.0653, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}


  5%|█████████▎                                                                                                                                                                               | 3/60 [01:39<30:42, 32.33s/it]
{'loss': 2.4491, 'grad_norm': nan, 'learning_rate': 4e-05, 'epoch': 0.01}

  7%|████████████▎                                                                                                                                                                            | 4/60 [02:02<26:40, 28.58s/it]


 10%|██████████████████▌                                                                                                                                                                      | 6/60 [02:47<22:37, 25.14s/it]

 12%|█████████████████████▌                                                                                                                                                                   | 7/60 [03:09<21:26, 24.27s/it]
{'loss': 2.2933, 'grad_norm': 3.9380502700805664, 'learning_rate': 0.0002, 'epoch': 0.02}

 13%|████████████████████████▋                                                                                                                                                                | 8/60 [03:32<20:32, 23.69s/it]

 15%|███████████████████████████▊                                                                                                                                                             | 9/60 [03:54<19:46, 23.27s/it]

 17%|██████████████████████████████▋                                                                                                                                                         | 10/60 [04:16<19:06, 22.92s/it]

 18%|█████████████████████████████████▋                                                                                                                                                      | 11/60 [04:38<18:30, 22.67s/it]

 20%|████████████████████████████████████▊                                                                                                                                                   | 12/60 [05:00<17:59, 22.49s/it]

 22%|███████████████████████████████████████▊                                                                                                                                                | 13/60 [05:22<17:26, 22.27s/it]


 25%|██████████████████████████████████████████████                                                                                                                                          | 15/60 [06:06<16:29, 21.98s/it]

 27%|█████████████████████████████████████████████████                                                                                                                                       | 16/60 [06:27<15:58, 21.79s/it]
{'loss': 1.7684, 'grad_norm': 9.963391304016113, 'learning_rate': 0.0001709090909090909, 'epoch': 0.04}


 30%|███████████████████████████████████████████████████████▏                                                                                                                                | 18/60 [07:10<15:04, 21.53s/it]
{'loss': 1.6767, 'grad_norm': 13.791019439697266, 'learning_rate': 0.00016363636363636366, 'epoch': 0.04}


 33%|█████████████████████████████████████████████████████████████▎                                                                                                                          | 20/60 [07:51<14:07, 21.18s/it]

 35%|████████████████████████████████████████████████████████████████▍                                                                                                                       | 21/60 [08:13<13:56, 21.45s/it]
{'loss': 1.4532, 'grad_norm': 9.891645431518555, 'learning_rate': 0.00015272727272727275, 'epoch': 0.05}


 38%|██████████████████████████████████████████████████████████████████████▌                                                                                                                 | 23/60 [09:04<14:23, 23.34s/it]
{'loss': 1.1352, 'grad_norm': 10.192435264587402, 'learning_rate': 0.00014545454545454546, 'epoch': 0.06}


 42%|████████████████████████████████████████████████████████████████████████████▋                                                                                                           | 25/60 [09:54<14:05, 24.17s/it]
{'loss': 0.8852, 'grad_norm': 8.991161346435547, 'learning_rate': 0.0001381818181818182, 'epoch': 0.06}


 45%|██████████████████████████████████████████████████████████████████████████████████▊                                                                                                     | 27/60 [10:58<15:32, 28.27s/it]
{'loss': 0.7855, 'grad_norm': 7.69948148727417, 'learning_rate': 0.00013090909090909093, 'epoch': 0.07}


 48%|████████████████████████████████████████████████████████████████████████████████████████▉                                                                                               | 29/60 [12:04<15:51, 30.69s/it]
{'loss': 1.0634, 'grad_norm': 8.704062461853027, 'learning_rate': 0.00012363636363636364, 'epoch': 0.07}


 52%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                         | 31/60 [13:09<15:17, 31.63s/it]

 53%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                     | 32/60 [13:41<14:54, 31.93s/it]
{'loss': 1.4679, 'grad_norm': nan, 'learning_rate': 0.00011636363636363636, 'epoch': 0.08}

 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                  | 33/60 [14:14<14:30, 32.25s/it]

 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                               | 34/60 [14:46<13:57, 32.20s/it]

 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                            | 35/60 [15:19<13:24, 32.16s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 187, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 172, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt