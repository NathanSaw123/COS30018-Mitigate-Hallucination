

  2%|███▎                                                                                                                                                                                                   | 1/60 [00:17<17:25, 17.73s/it]
{'loss': 2.6786, 'grad_norm': 1.2756493091583252, 'learning_rate': 4e-05, 'epoch': 0.0}

  3%|██████▋                                                                                                                                                                                                | 2/60 [01:34<50:45, 52.51s/it]

  5%|█████████▉                                                                                                                                                                                             | 3/60 [02:21<47:15, 49.75s/it]

  7%|█████████████▎                                                                                                                                                                                         | 4/60 [03:26<52:20, 56.08s/it]


 10%|███████████████████▉                                                                                                                                                                                   | 6/60 [04:53<44:13, 49.14s/it]
{'loss': 1.9132, 'grad_norm': 0.9972823858261108, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}

 12%|███████████████████████▏                                                                                                                                                                               | 7/60 [05:37<41:44, 47.26s/it]

 13%|██████████████████████████▌                                                                                                                                                                            | 8/60 [06:32<43:18, 49.97s/it]

 15%|█████████████████████████████▊                                                                                                                                                                         | 9/60 [07:00<36:28, 42.92s/it]


 18%|████████████████████████████████████▎                                                                                                                                                                 | 11/60 [09:17<43:54, 53.77s/it]
{'loss': 1.9027, 'grad_norm': 2.0911214351654053, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}

 20%|███████████████████████████████████████▌                                                                                                                                                              | 12/60 [09:51<38:02, 47.55s/it]

 22%|██████████████████████████████████████████▉                                                                                                                                                           | 13/60 [10:42<38:15, 48.85s/it]

 23%|██████████████████████████████████████████████▏                                                                                                                                                       | 14/60 [11:19<34:40, 45.23s/it]


 27%|████████████████████████████████████████████████████▊                                                                                                                                                 | 16/60 [12:36<31:43, 43.26s/it]

 28%|████████████████████████████████████████████████████████                                                                                                                                              | 17/60 [13:24<32:02, 44.72s/it]
{'loss': 1.6552, 'grad_norm': 1.960067868232727, 'learning_rate': 0.00015636363636363637, 'epoch': 0.0}

 30%|███████████████████████████████████████████████████████████▍                                                                                                                                          | 18/60 [13:44<26:09, 37.36s/it]


 33%|██████████████████████████████████████████████████████████████████                                                                                                                                    | 20/60 [15:26<30:12, 45.31s/it]
{'loss': 1.3591, 'grad_norm': 2.222538709640503, 'learning_rate': 0.00014545454545454546, 'epoch': 0.0}


 37%|████████████████████████████████████████████████████████████████████████▌                                                                                                                             | 22/60 [16:32<23:55, 37.79s/it]

 38%|███████████████████████████████████████████████████████████████████████████▉                                                                                                                          | 23/60 [17:11<23:41, 38.42s/it]

 40%|███████████████████████████████████████████████████████████████████████████████▏                                                                                                                      | 24/60 [17:49<22:58, 38.30s/it]
{'loss': 1.2371, 'grad_norm': 2.18037486076355, 'learning_rate': 0.00013090909090909093, 'epoch': 0.01}


 43%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                | 26/60 [19:16<22:46, 40.20s/it]
{'loss': 1.4725, 'grad_norm': 1.3651689291000366, 'learning_rate': 0.00012363636363636364, 'epoch': 0.01}

 45%|█████████████████████████████████████████████████████████████████████████████████████████                                                                                                             | 27/60 [20:04<23:31, 42.77s/it]

 47%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                         | 28/60 [20:32<20:26, 38.32s/it]

 48%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                      | 29/60 [20:47<16:09, 31.28s/it]


 52%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                               | 31/60 [21:52<15:57, 33.03s/it]
{'loss': 1.2857, 'grad_norm': 0.9715902805328369, 'learning_rate': 0.00010545454545454545, 'epoch': 0.01}

 53%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                            | 32/60 [22:25<15:25, 33.05s/it]


 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                     | 34/60 [23:54<17:20, 40.01s/it]

 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                  | 35/60 [24:28<15:56, 38.27s/it]

 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                               | 36/60 [25:24<17:24, 43.53s/it]
{'loss': 1.2743, 'grad_norm': 1.1280080080032349, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.01}

 62%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                            | 37/60 [26:05<16:20, 42.65s/it]


 65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 39/60 [27:16<13:24, 38.33s/it]

 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                  | 40/60 [28:16<14:55, 44.76s/it]
{'loss': 1.3058, 'grad_norm': 1.0725990533828735, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.01}

 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 41/60 [28:48<13:01, 41.12s/it]

 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 42/60 [29:37<13:03, 43.52s/it]

 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                        | 43/60 [30:21<12:21, 43.60s/it]


 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                 | 45/60 [31:38<10:05, 40.38s/it]
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                 | 45/60 [31:38<10:05, 40.38s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 132, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 118, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1208, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 41.46 GiB is allocated by PyTorch, and 718.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)