{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.03900853311661926,
  "eval_steps": 500,
  "global_step": 120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003250711093051605,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 1.6429,
      "step": 1
    },
    {
      "epoch": 0.000650142218610321,
      "grad_norm": 2.710452079772949,
      "learning_rate": 4e-05,
      "loss": 1.6663,
      "step": 2
    },
    {
      "epoch": 0.0009752133279154815,
      "grad_norm": 2.670156240463257,
      "learning_rate": 8e-05,
      "loss": 1.4778,
      "step": 3
    },
    {
      "epoch": 0.001300284437220642,
      "grad_norm": 2.6475746631622314,
      "learning_rate": 0.00012,
      "loss": 1.5659,
      "step": 4
    },
    {
      "epoch": 0.0016253555465258025,
      "grad_norm": 2.933476686477661,
      "learning_rate": 0.00016,
      "loss": 1.8775,
      "step": 5
    },
    {
      "epoch": 0.001950426655830963,
      "grad_norm": 2.8258113861083984,
      "learning_rate": 0.0002,
      "loss": 1.6763,
      "step": 6
    },
    {
      "epoch": 0.0022754977651361233,
      "grad_norm": 2.5661027431488037,
      "learning_rate": 0.0001982608695652174,
      "loss": 1.5859,
      "step": 7
    },
    {
      "epoch": 0.002600568874441284,
      "grad_norm": 5.792759418487549,
      "learning_rate": 0.0001965217391304348,
      "loss": 1.6166,
      "step": 8
    },
    {
      "epoch": 0.0029256399837464444,
      "grad_norm": 4.653321743011475,
      "learning_rate": 0.00019478260869565218,
      "loss": 1.5544,
      "step": 9
    },
    {
      "epoch": 0.003250711093051605,
      "grad_norm": 3.048267126083374,
      "learning_rate": 0.00019304347826086958,
      "loss": 1.7424,
      "step": 10
    },
    {
      "epoch": 0.0035757822023567655,
      "grad_norm": 3.853036403656006,
      "learning_rate": 0.00019130434782608697,
      "loss": 1.7745,
      "step": 11
    },
    {
      "epoch": 0.003900853311661926,
      "grad_norm": 4.3646135330200195,
      "learning_rate": 0.00018956521739130436,
      "loss": 1.5668,
      "step": 12
    },
    {
      "epoch": 0.0042259244209670865,
      "grad_norm": 2.9504990577697754,
      "learning_rate": 0.00018782608695652175,
      "loss": 1.4255,
      "step": 13
    },
    {
      "epoch": 0.004550995530272247,
      "grad_norm": NaN,
      "learning_rate": 0.00018782608695652175,
      "loss": 1.5149,
      "step": 14
    },
    {
      "epoch": 0.004876066639577408,
      "grad_norm": 3.859309196472168,
      "learning_rate": 0.00018608695652173914,
      "loss": 1.416,
      "step": 15
    },
    {
      "epoch": 0.005201137748882568,
      "grad_norm": 4.187564849853516,
      "learning_rate": 0.00018434782608695653,
      "loss": 1.5827,
      "step": 16
    },
    {
      "epoch": 0.005526208858187729,
      "grad_norm": 6.023655414581299,
      "learning_rate": 0.00018260869565217392,
      "loss": 1.5365,
      "step": 17
    },
    {
      "epoch": 0.005851279967492889,
      "grad_norm": 6.239261627197266,
      "learning_rate": 0.00018086956521739132,
      "loss": 1.5303,
      "step": 18
    },
    {
      "epoch": 0.00617635107679805,
      "grad_norm": 7.7479047775268555,
      "learning_rate": 0.0001791304347826087,
      "loss": 1.5039,
      "step": 19
    },
    {
      "epoch": 0.00650142218610321,
      "grad_norm": 6.356286525726318,
      "learning_rate": 0.0001773913043478261,
      "loss": 1.4961,
      "step": 20
    },
    {
      "epoch": 0.006826493295408371,
      "grad_norm": 6.315685272216797,
      "learning_rate": 0.0001756521739130435,
      "loss": 1.5801,
      "step": 21
    },
    {
      "epoch": 0.007151564404713531,
      "grad_norm": 6.402100086212158,
      "learning_rate": 0.00017391304347826088,
      "loss": 1.3943,
      "step": 22
    },
    {
      "epoch": 0.007476635514018692,
      "grad_norm": 6.178267478942871,
      "learning_rate": 0.00017217391304347827,
      "loss": 1.7136,
      "step": 23
    },
    {
      "epoch": 0.007801706623323852,
      "grad_norm": 6.047845363616943,
      "learning_rate": 0.00017043478260869566,
      "loss": 1.5946,
      "step": 24
    },
    {
      "epoch": 0.008126777732629013,
      "grad_norm": 5.331643581390381,
      "learning_rate": 0.00016869565217391306,
      "loss": 1.6526,
      "step": 25
    },
    {
      "epoch": 0.008451848841934173,
      "grad_norm": 7.110306739807129,
      "learning_rate": 0.00016695652173913042,
      "loss": 1.5297,
      "step": 26
    },
    {
      "epoch": 0.008776919951239333,
      "grad_norm": 7.559707164764404,
      "learning_rate": 0.00016521739130434784,
      "loss": 1.4217,
      "step": 27
    },
    {
      "epoch": 0.009101991060544493,
      "grad_norm": 6.141975402832031,
      "learning_rate": 0.00016347826086956523,
      "loss": 1.2146,
      "step": 28
    },
    {
      "epoch": 0.009427062169849655,
      "grad_norm": 4.901401996612549,
      "learning_rate": 0.00016173913043478262,
      "loss": 1.5522,
      "step": 29
    },
    {
      "epoch": 0.009752133279154815,
      "grad_norm": 5.940823554992676,
      "learning_rate": 0.00016,
      "loss": 1.5502,
      "step": 30
    },
    {
      "epoch": 0.010077204388459975,
      "grad_norm": NaN,
      "learning_rate": 0.00016,
      "loss": 1.2515,
      "step": 31
    },
    {
      "epoch": 0.010402275497765135,
      "grad_norm": 5.681644916534424,
      "learning_rate": 0.0001582608695652174,
      "loss": 1.2618,
      "step": 32
    },
    {
      "epoch": 0.010727346607070297,
      "grad_norm": 5.75717830657959,
      "learning_rate": 0.0001565217391304348,
      "loss": 1.2901,
      "step": 33
    },
    {
      "epoch": 0.011052417716375457,
      "grad_norm": 7.755176067352295,
      "learning_rate": 0.0001547826086956522,
      "loss": 1.3235,
      "step": 34
    },
    {
      "epoch": 0.011377488825680617,
      "grad_norm": 4.3030500411987305,
      "learning_rate": 0.00015304347826086958,
      "loss": 1.4801,
      "step": 35
    },
    {
      "epoch": 0.011702559934985778,
      "grad_norm": 6.309263706207275,
      "learning_rate": 0.00015130434782608694,
      "loss": 1.1669,
      "step": 36
    },
    {
      "epoch": 0.01202763104429094,
      "grad_norm": 5.367025852203369,
      "learning_rate": 0.00014956521739130436,
      "loss": 1.6168,
      "step": 37
    },
    {
      "epoch": 0.0123527021535961,
      "grad_norm": 7.615872383117676,
      "learning_rate": 0.00014782608695652173,
      "loss": 1.5473,
      "step": 38
    },
    {
      "epoch": 0.01267777326290126,
      "grad_norm": 7.843869686126709,
      "learning_rate": 0.00014608695652173914,
      "loss": 1.6614,
      "step": 39
    },
    {
      "epoch": 0.01300284437220642,
      "grad_norm": 7.81007719039917,
      "learning_rate": 0.00014434782608695654,
      "loss": 1.6889,
      "step": 40
    },
    {
      "epoch": 0.01332791548151158,
      "grad_norm": 6.988715171813965,
      "learning_rate": 0.00014260869565217393,
      "loss": 1.3569,
      "step": 41
    },
    {
      "epoch": 0.013652986590816742,
      "grad_norm": 7.834819316864014,
      "learning_rate": 0.00014086956521739132,
      "loss": 1.4461,
      "step": 42
    },
    {
      "epoch": 0.013978057700121902,
      "grad_norm": NaN,
      "learning_rate": 0.00014086956521739132,
      "loss": 1.2358,
      "step": 43
    },
    {
      "epoch": 0.014303128809427062,
      "grad_norm": 8.656362533569336,
      "learning_rate": 0.0001391304347826087,
      "loss": 1.5298,
      "step": 44
    },
    {
      "epoch": 0.014628199918732222,
      "grad_norm": 10.078700065612793,
      "learning_rate": 0.0001373913043478261,
      "loss": 1.479,
      "step": 45
    },
    {
      "epoch": 0.014953271028037384,
      "grad_norm": 12.684611320495605,
      "learning_rate": 0.00013565217391304347,
      "loss": 0.8871,
      "step": 46
    },
    {
      "epoch": 0.015278342137342544,
      "grad_norm": 7.579677104949951,
      "learning_rate": 0.00013391304347826088,
      "loss": 1.1711,
      "step": 47
    },
    {
      "epoch": 0.015603413246647704,
      "grad_norm": 10.194597244262695,
      "learning_rate": 0.00013217391304347825,
      "loss": 1.4979,
      "step": 48
    },
    {
      "epoch": 0.015928484355952864,
      "grad_norm": 15.636857986450195,
      "learning_rate": 0.00013043478260869567,
      "loss": 1.3371,
      "step": 49
    },
    {
      "epoch": 0.016253555465258026,
      "grad_norm": 14.237262725830078,
      "learning_rate": 0.00012869565217391303,
      "loss": 1.3891,
      "step": 50
    },
    {
      "epoch": 0.016578626574563184,
      "grad_norm": 6.709916114807129,
      "learning_rate": 0.00012695652173913045,
      "loss": 1.2533,
      "step": 51
    },
    {
      "epoch": 0.016903697683868346,
      "grad_norm": 3.505483865737915,
      "learning_rate": 0.00012521739130434784,
      "loss": 1.3718,
      "step": 52
    },
    {
      "epoch": 0.017228768793173508,
      "grad_norm": 3.5389394760131836,
      "learning_rate": 0.00012347826086956523,
      "loss": 1.1599,
      "step": 53
    },
    {
      "epoch": 0.017553839902478666,
      "grad_norm": 3.250338077545166,
      "learning_rate": 0.00012173913043478263,
      "loss": 1.1823,
      "step": 54
    },
    {
      "epoch": 0.017878911011783828,
      "grad_norm": 4.1795125007629395,
      "learning_rate": 0.00012,
      "loss": 0.856,
      "step": 55
    },
    {
      "epoch": 0.018203982121088987,
      "grad_norm": 6.092467784881592,
      "learning_rate": 0.00011826086956521741,
      "loss": 0.782,
      "step": 56
    },
    {
      "epoch": 0.01852905323039415,
      "grad_norm": 4.732769966125488,
      "learning_rate": 0.00011652173913043479,
      "loss": 0.939,
      "step": 57
    },
    {
      "epoch": 0.01885412433969931,
      "grad_norm": 4.570638179779053,
      "learning_rate": 0.00011478260869565218,
      "loss": 1.0662,
      "step": 58
    },
    {
      "epoch": 0.01917919544900447,
      "grad_norm": 5.187560081481934,
      "learning_rate": 0.00011304347826086956,
      "loss": 0.8697,
      "step": 59
    },
    {
      "epoch": 0.01950426655830963,
      "grad_norm": 2.8727192878723145,
      "learning_rate": 0.00011130434782608696,
      "loss": 1.1461,
      "step": 60
    },
    {
      "epoch": 0.019829337667614792,
      "grad_norm": 6.8379106521606445,
      "learning_rate": 0.00010956521739130434,
      "loss": 0.4916,
      "step": 61
    },
    {
      "epoch": 0.02015440877691995,
      "grad_norm": 5.327408790588379,
      "learning_rate": 0.00010782608695652174,
      "loss": 0.6226,
      "step": 62
    },
    {
      "epoch": 0.020479479886225112,
      "grad_norm": 3.307368278503418,
      "learning_rate": 0.00010608695652173915,
      "loss": 0.844,
      "step": 63
    },
    {
      "epoch": 0.02080455099553027,
      "grad_norm": 3.18034029006958,
      "learning_rate": 0.00010434782608695653,
      "loss": 0.8564,
      "step": 64
    },
    {
      "epoch": 0.021129622104835433,
      "grad_norm": 3.2372913360595703,
      "learning_rate": 0.00010260869565217393,
      "loss": 1.0359,
      "step": 65
    },
    {
      "epoch": 0.021454693214140595,
      "grad_norm": 4.070638656616211,
      "learning_rate": 0.00010086956521739131,
      "loss": 1.4887,
      "step": 66
    },
    {
      "epoch": 0.021779764323445753,
      "grad_norm": 3.635181188583374,
      "learning_rate": 9.91304347826087e-05,
      "loss": 1.3351,
      "step": 67
    },
    {
      "epoch": 0.022104835432750915,
      "grad_norm": 3.5477492809295654,
      "learning_rate": 9.739130434782609e-05,
      "loss": 1.3261,
      "step": 68
    },
    {
      "epoch": 0.022429906542056073,
      "grad_norm": 3.1665239334106445,
      "learning_rate": 9.565217391304348e-05,
      "loss": 1.6342,
      "step": 69
    },
    {
      "epoch": 0.022754977651361235,
      "grad_norm": 4.253606796264648,
      "learning_rate": 9.391304347826087e-05,
      "loss": 1.2692,
      "step": 70
    },
    {
      "epoch": 0.023080048760666397,
      "grad_norm": 3.2710044384002686,
      "learning_rate": 9.217391304347827e-05,
      "loss": 1.42,
      "step": 71
    },
    {
      "epoch": 0.023405119869971555,
      "grad_norm": 5.197497844696045,
      "learning_rate": 9.043478260869566e-05,
      "loss": 1.5714,
      "step": 72
    },
    {
      "epoch": 0.023730190979276717,
      "grad_norm": 4.175668716430664,
      "learning_rate": 8.869565217391305e-05,
      "loss": 1.3755,
      "step": 73
    },
    {
      "epoch": 0.02405526208858188,
      "grad_norm": 3.785188674926758,
      "learning_rate": 8.695652173913044e-05,
      "loss": 1.3314,
      "step": 74
    },
    {
      "epoch": 0.024380333197887037,
      "grad_norm": 3.8095486164093018,
      "learning_rate": 8.521739130434783e-05,
      "loss": 1.2204,
      "step": 75
    },
    {
      "epoch": 0.0247054043071922,
      "grad_norm": 5.234846591949463,
      "learning_rate": 8.347826086956521e-05,
      "loss": 1.3474,
      "step": 76
    },
    {
      "epoch": 0.025030475416497357,
      "grad_norm": 4.514505386352539,
      "learning_rate": 8.173913043478262e-05,
      "loss": 1.4543,
      "step": 77
    },
    {
      "epoch": 0.02535554652580252,
      "grad_norm": 4.773910045623779,
      "learning_rate": 8e-05,
      "loss": 1.5825,
      "step": 78
    },
    {
      "epoch": 0.02568061763510768,
      "grad_norm": 3.893955945968628,
      "learning_rate": 7.82608695652174e-05,
      "loss": 1.4181,
      "step": 79
    },
    {
      "epoch": 0.02600568874441284,
      "grad_norm": 5.4065022468566895,
      "learning_rate": 7.652173913043479e-05,
      "loss": 1.1426,
      "step": 80
    },
    {
      "epoch": 0.026330759853718,
      "grad_norm": 5.693452835083008,
      "learning_rate": 7.478260869565218e-05,
      "loss": 1.5288,
      "step": 81
    },
    {
      "epoch": 0.02665583096302316,
      "grad_norm": 4.29986572265625,
      "learning_rate": 7.304347826086957e-05,
      "loss": 1.4358,
      "step": 82
    },
    {
      "epoch": 0.02698090207232832,
      "grad_norm": 6.854045867919922,
      "learning_rate": 7.130434782608696e-05,
      "loss": 0.9358,
      "step": 83
    },
    {
      "epoch": 0.027305973181633483,
      "grad_norm": 4.9938178062438965,
      "learning_rate": 6.956521739130436e-05,
      "loss": 1.4989,
      "step": 84
    },
    {
      "epoch": 0.02763104429093864,
      "grad_norm": 5.657486915588379,
      "learning_rate": 6.782608695652173e-05,
      "loss": 1.1143,
      "step": 85
    },
    {
      "epoch": 0.027956115400243804,
      "grad_norm": 5.54247522354126,
      "learning_rate": 6.608695652173912e-05,
      "loss": 1.3287,
      "step": 86
    },
    {
      "epoch": 0.028281186509548965,
      "grad_norm": 6.6121063232421875,
      "learning_rate": 6.434782608695652e-05,
      "loss": 1.1029,
      "step": 87
    },
    {
      "epoch": 0.028606257618854124,
      "grad_norm": 6.210025787353516,
      "learning_rate": 6.260869565217392e-05,
      "loss": 1.4521,
      "step": 88
    },
    {
      "epoch": 0.028931328728159286,
      "grad_norm": 7.688536643981934,
      "learning_rate": 6.086956521739131e-05,
      "loss": 1.0436,
      "step": 89
    },
    {
      "epoch": 0.029256399837464444,
      "grad_norm": 6.670773506164551,
      "learning_rate": 5.9130434782608704e-05,
      "loss": 1.2232,
      "step": 90
    },
    {
      "epoch": 0.029581470946769606,
      "grad_norm": 6.948412895202637,
      "learning_rate": 5.739130434782609e-05,
      "loss": 0.9874,
      "step": 91
    },
    {
      "epoch": 0.029906542056074768,
      "grad_norm": 7.507504463195801,
      "learning_rate": 5.565217391304348e-05,
      "loss": 1.6587,
      "step": 92
    },
    {
      "epoch": 0.030231613165379926,
      "grad_norm": 7.420435905456543,
      "learning_rate": 5.391304347826087e-05,
      "loss": 1.0664,
      "step": 93
    },
    {
      "epoch": 0.030556684274685088,
      "grad_norm": 8.706086158752441,
      "learning_rate": 5.217391304347826e-05,
      "loss": 1.3617,
      "step": 94
    },
    {
      "epoch": 0.030881755383990246,
      "grad_norm": 8.003230094909668,
      "learning_rate": 5.0434782608695655e-05,
      "loss": 1.1878,
      "step": 95
    },
    {
      "epoch": 0.031206826493295408,
      "grad_norm": 7.093264579772949,
      "learning_rate": 4.8695652173913046e-05,
      "loss": 1.0191,
      "step": 96
    },
    {
      "epoch": 0.03153189760260057,
      "grad_norm": 9.385339736938477,
      "learning_rate": 4.695652173913044e-05,
      "loss": 1.1911,
      "step": 97
    },
    {
      "epoch": 0.03185696871190573,
      "grad_norm": 8.689809799194336,
      "learning_rate": 4.521739130434783e-05,
      "loss": 1.3181,
      "step": 98
    },
    {
      "epoch": 0.03218203982121089,
      "grad_norm": 14.091991424560547,
      "learning_rate": 4.347826086956522e-05,
      "loss": 1.0057,
      "step": 99
    },
    {
      "epoch": 0.03250711093051605,
      "grad_norm": 11.336821556091309,
      "learning_rate": 4.1739130434782605e-05,
      "loss": 1.3214,
      "step": 100
    },
    {
      "epoch": 0.03283218203982121,
      "grad_norm": 3.801746368408203,
      "learning_rate": 4e-05,
      "loss": 1.1594,
      "step": 101
    },
    {
      "epoch": 0.03315725314912637,
      "grad_norm": 4.146900653839111,
      "learning_rate": 3.8260869565217395e-05,
      "loss": 1.1225,
      "step": 102
    },
    {
      "epoch": 0.033482324258431534,
      "grad_norm": 3.512681722640991,
      "learning_rate": 3.6521739130434786e-05,
      "loss": 1.0986,
      "step": 103
    },
    {
      "epoch": 0.03380739536773669,
      "grad_norm": 3.9103949069976807,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.0219,
      "step": 104
    },
    {
      "epoch": 0.03413246647704185,
      "grad_norm": 3.006814956665039,
      "learning_rate": 3.304347826086956e-05,
      "loss": 1.133,
      "step": 105
    },
    {
      "epoch": 0.034457537586347016,
      "grad_norm": 3.428990602493286,
      "learning_rate": 3.130434782608696e-05,
      "loss": 0.7767,
      "step": 106
    },
    {
      "epoch": 0.034782608695652174,
      "grad_norm": 3.199350595474243,
      "learning_rate": 2.9565217391304352e-05,
      "loss": 0.4483,
      "step": 107
    },
    {
      "epoch": 0.03510767980495733,
      "grad_norm": 2.3000669479370117,
      "learning_rate": 2.782608695652174e-05,
      "loss": 1.0721,
      "step": 108
    },
    {
      "epoch": 0.0354327509142625,
      "grad_norm": 2.7370874881744385,
      "learning_rate": 2.608695652173913e-05,
      "loss": 0.8938,
      "step": 109
    },
    {
      "epoch": 0.035757822023567656,
      "grad_norm": 2.5789687633514404,
      "learning_rate": 2.4347826086956523e-05,
      "loss": 1.1091,
      "step": 110
    },
    {
      "epoch": 0.036082893132872815,
      "grad_norm": 2.9445533752441406,
      "learning_rate": 2.2608695652173914e-05,
      "loss": 0.8525,
      "step": 111
    },
    {
      "epoch": 0.03640796424217797,
      "grad_norm": 3.3760476112365723,
      "learning_rate": 2.0869565217391303e-05,
      "loss": 0.6078,
      "step": 112
    },
    {
      "epoch": 0.03673303535148314,
      "grad_norm": 2.5729613304138184,
      "learning_rate": 1.9130434782608697e-05,
      "loss": 1.05,
      "step": 113
    },
    {
      "epoch": 0.0370581064607883,
      "grad_norm": 2.3530187606811523,
      "learning_rate": 1.739130434782609e-05,
      "loss": 0.9289,
      "step": 114
    },
    {
      "epoch": 0.037383177570093455,
      "grad_norm": 3.393033027648926,
      "learning_rate": 1.565217391304348e-05,
      "loss": 0.7919,
      "step": 115
    },
    {
      "epoch": 0.03770824867939862,
      "grad_norm": 2.369497060775757,
      "learning_rate": 1.391304347826087e-05,
      "loss": 0.7576,
      "step": 116
    },
    {
      "epoch": 0.03803331978870378,
      "grad_norm": 2.8132383823394775,
      "learning_rate": 1.2173913043478261e-05,
      "loss": 0.9498,
      "step": 117
    },
    {
      "epoch": 0.03835839089800894,
      "grad_norm": 2.8736469745635986,
      "learning_rate": 1.0434782608695651e-05,
      "loss": 1.17,
      "step": 118
    },
    {
      "epoch": 0.0386834620073141,
      "grad_norm": 3.5284438133239746,
      "learning_rate": 8.695652173913044e-06,
      "loss": 1.403,
      "step": 119
    },
    {
      "epoch": 0.03900853311661926,
      "grad_norm": 4.4773268699646,
      "learning_rate": 6.956521739130435e-06,
      "loss": 1.2832,
      "step": 120
    }
  ],
  "logging_steps": 1,
  "max_steps": 120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5972517880823808.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
